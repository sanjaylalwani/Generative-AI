### Basic Generative AI 
1. What is the difference between generative and discriminative models?
2. What is catastrophic forgetting in the context of LLMs?
3. What is tokenization? What are different tokenization methods?
4. Compare the word embeddings and sentence embeddings. How do their applications differ, and what considerations come into play when choosing between them?
5. What is Hallucination? Why Hallucination occurs?
6. How can you evaluate and mitigate Hallucination?
7. What are SLMs (Small Language Models)?
8. What are the benefits and drawbacks of SLMs?

### LLM and different architecure
1. Explain the Transformers architecture?
2. How does self-attention work?
3. Why is positional encoding essential in transformer models, and what problem does it solve in self-attention mechanisms?
4. How and why are transformers better than RNN architectures?
5. What are different LLM architectures?
6. How does Sequence to Sequence model works?

### Prompt Engineering
1. What is prompt engineering?
2. What are different Prompting techniques?
3. What is meta prompting?
4. How do you evaluate the effectiveness of a prompt?

### RAG
1. What is RAG? Why they are so important?
2. What are the benefits of Retrieval-Augmented Generation?
3. What are some advanced techniques to imporvise RAG performance?
4. What are the different chunking techniques, and what are their pros and cons?
5. What is late chunking and how is it different from traditional chunking methods?

### Fine Tuning
1. What is fine-tuning in LLMs?
2. What is the need for fine tuning LLMs?
3. What is the difference between fine tuning and training LLMs?
4. What is PEFT LoRA in Fine tuning?

### Evaluation (technique, metrics)
1. What are the different evaluation techniques used to evaluate LLM Application?
2. What is Perplexity?
3. What is BLEU?
4. What is ROUGE?
5. What is RAGAS?
   
### Deployment and Monitoring
1. How do you optimize AI models to balance performance and computational cost in production systems?
2. What is RLHF, how is it used?

