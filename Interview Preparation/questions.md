1. What is the difference between generative and discriminative models?
2. Explain the Transformers architecture?
3. How does self-attention work?
4. Why is positional encoding essential in transformer models, and what problem does it solve in self-attention mechanisms?
5. How and why are transformers better than RNN architectures?
6. What is Hallucination? Why Hallucination occurs?
7. How can you evaluate and mitigate Hallucination?
8. What is catastrophic forgetting in the context of LLMs?
9. What is RAG? Why they are so important?
10. What are the benefits of Retrieval-Augmented Generation?
11. What are some advanced techniques to imporvise RAG performance?
12. What are different LLM architectures?
13. What is prompt engineering?
14. What are different Prompting techniques?
15. What is fine-tuning in LLMs?
16. What is the need for fine tuning LLMs?
17. What is the difference between fine tuning and training LLMs?
18. What is PEFT LoRA in Fine tuning?
19. What are SLMs (Small Language Models)?
20. What are the benefits and drawbacks of SLMs?
