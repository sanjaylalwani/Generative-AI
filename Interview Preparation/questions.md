1. What is the difference between generative and discriminative models?
2. Explain the Transformers architecture?
3. How does self-attention work?
4. Why is positional encoding essential in transformer models, and what problem does it solve in self-attention mechanisms?
5. How and why are transformers better than RNN architectures?
6. What is Hallucination? Why Hallucination occurs?
7. How can you evaluate and mitigate Hallucination?
8. What is catastrophic forgetting in the context of LLMs?
9. What is RAG? Why they are so important?
10. What are the benefits of Retrieval-Augmented Generation?
11. What are some advanced techniques to imporvise RAG performance?
12. What are different LLM architectures?
13. How does Sequence to Sequence model works?
14. What is tokenization? What are different tokenization methods?
15. Compare the word embeddings and sentence embeddings. How do their applications differ, and what considerations come into play when choosing between them?
16. What is prompt engineering?
17. What are different Prompting techniques?
18. What is meta prompting?
19. What is fine-tuning in LLMs?
20. What is the need for fine tuning LLMs?
21. What is the difference between fine tuning and training LLMs?
22. What is PEFT LoRA in Fine tuning?
23. What are SLMs (Small Language Models)?
24. What are the benefits and drawbacks of SLMs?
25. How do you optimize AI models to balance performance and computational cost in production systems?
26. What is RLHF, how is it used?
